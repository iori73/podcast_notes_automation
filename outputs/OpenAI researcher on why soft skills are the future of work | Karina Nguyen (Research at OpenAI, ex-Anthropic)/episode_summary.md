## **基本情報**

- Spotify URL：[エピソードリンク](https://open.spotify.com/episode/15U5deuVOOYj8YM0Q5AS9N?si=92db0af2429941cf&nd=1&dlsi=43b32d3ecd46429e)
- 公開日：2025年02月09日
- 長さ：74:33
- LISTEN URL：

## **要約**



## **目次**


## **文字起こし**

Not only are you working at the cutting edge of AI and LLMs, you're actually building the cutting edge. When I first came to Andarban, I was like, oh my god, I really love front-end engineering. And then the reason why I switched to research is because I realized, oh my god, cloud is getting better at front-end. Cloud is getting better at like coding. I think cloud can like develop new apps. What skills do you think will be most valuable going forward for product teams in particular? Creative thinking. You kind of want to like generate a bunch of ideas and like filter through them in order to build the best product experience. I think it's actually really, really hard to teach the model how to be aesthetic or really good visual design, or like how to be extremely creative in the way they write. What do you think people most misunderstand about how models are created? When you taught the model some of the self-knowledge of you actually don't have a physical body to operate in the physical world, the model would get like extremely confused. Today, my guest is Karina Nguyen. Karina is an AI researcher at OpenAI, where she helped build Canvas, Tasks, the O1 Chain of Thought model, and more. Prior to OpenAI, she was at Anthropic, where she led work on post-training and evaluation for the Cloud 3 models, built the document upload feature with 100k context windows, and so much more. She was also an engineer at New York Times, was a designer at Dropbox and at Square. It's very rare to get a glimpse into how someone working on the bleeding edge of AI and LLMs operates and how they think about where things are heading. In our conversation, we talk about how teams at OpenAI operate and build product, what skills she thinks you should be building as AI gets smarter, how models are created, why synthetic data will allow models to keep getting smarter, and why she moved from engineering to research after realizing how good LLMs are going to be at coding. If you enjoy this podcast, don't forget to subscribe and follow it in your favorite podcasting app or YouTube. It's the best way to avoid missing future episodes, and it helps the podcast tremendously. With that, I bring you Karina Nguyen. This episode is brought to you by Interpret. Interpret unifies all your customer interactions, from Gong calls to Zendesk tickets to Twitter threads to App Store reviews, and makes it available for analysis. It's trusted by leading product orgs like Canva, Notion, Loom, Linear, Monday.com, and Strava to bring the voice of the customer into the product development process, helping you build best-in-class products faster. What makes Interpret special is its ability to build and update customer-specific AI models that provide the most granular and accurate insights into your business, connect customer insights to revenue and operational data in your CRM or data warehouse to map the business impact of each customer need and prioritize confidently, and empower your entire team to easily take action on use cases like win-loss analysis, critical bug detection, and identifying drivers of churn with Interpret's AI assistant, Wisdom. Looking to automate your feedback loops and prioritize your roadmap with confidence like Notion, Canva, and Linear? Visit enterpret.com slash Lenny to connect with the team and get two free months when you sign up for an annual plan. This is a limited-time offer. That's interpret.com slash Lenny. This episode is brought to you by Vanta, and I'm very excited to have Cristina Cacioppo, CEO and co-founder of Vanta, joining me for this very short conversation. Great to be here. Big fan of the podcast and the newsletter. Vanta is a longtime sponsor of the show, but for some of our newer listeners, what does Vanta do and who is it for? Sure. So we started Vanta in 2018 focused on founders, helping them start to build out their security programs and get credit for all of that hard security work with compliance certifications like SOC 2 or ISO 2701. Today, we currently help over 9,000 companies, including some startup household names like Atlassian, Ramp, and LangChain start and scale their security programs and ultimately build trust by automating compliance, centralizing GRC, and accelerating security reviews. That is awesome. I know from experience that these things take a lot of time and a lot of resources, and nobody wants to spend time doing this. That is very much our experience, but before the company, and to some extent during it, but the idea is with automation, with AI, with software, we are helping customers build trust with prospects and customers in an efficient way. And our joke, we started this compliance company so you don't have to. We appreciate you for doing that. And you have a special discount for listeners. They can get $1,000 off Vanta at Vanta.com slash Lenny. That's V-A-N-T-A.com slash Lenny for $1,000 off Vanta. Thanks for that, Christina. Thank you. Karina, thank you so much for being here. Welcome to the podcast. Thank you so much, Lenny, for inviting me. I'm very excited to have you here because not only are you working at the cutting edge of AI and LLMs, you're actually building the cutting edge of AI and LLMs. You recently launched this feature, which is basically the first agent feature of OpenAI. I also just did this survey. I don't know if you know about this. I did a survey of my readers and asked them, what tools do you use every day in your work and most used? And ChatGPT was number one above Gmail, above Slack, above anything else. 90% of people said they use ChatGPT regularly. It's absurd. It wasn't around two years ago. No. Also, we're recording this the week that OpenAI announced Stargate, which is this half trillion dollar investment in AI infrastructure. So there's just a lot happening constantly in AI, and you have a really unique glimpse into how things are working, where things are going, how work gets done. So I have a lot of questions for you. I want to talk about how you operate and how you work at OpenAI, where you think things are going, what skills are going to matter more and less in the future, and also just where things are going broadly. So how does that sound? Sounds great. Thank you so much. Yeah, I was extremely lucky.to join early days on topic and kind of learned a lot of things there. And I joined OpenAI around like eight months ago. So yeah, I'm excited to dive more into it. Okay, I'm going to definitely ask you about the differences between those, but I want to start more technical and just dive right in. I want to talk about model training. People always hear about models being trained. These big models, how much data it takes, how long it takes, how much money you toss it takes, how we're running out of data, which I want to talk about. Let me just ask you this question. What do you think people most misunderstand about how models are created? Model training is more an art than a science. And in a lot of ways, like we as like model trainers think a lot about like data quality is like it's one of the most important things in model training is like, how do you ensure the highest quality data for certain like interaction model behavior that you want to create? But the way you debug models is actually very similar the way you debug software. So one of the things that I've learned early days at Anthropic was like, we've discovered, especially with like Cloud 3D training, when you taught the model some of the self-knowledge of like, hey, like you actually don't have a physical body to operate like in the physical world. But then at the same time, we had data that kind of taught the model some of the function calls, which is like, this is how you set the alarm. And so the model would get like extremely confused about like, whether it can set an alarm and it doesn't have a body in the physical world. So it's like the model gets confused and sometimes it's like over refused. So sometimes it's like, I don't know, like, sorry, I cannot help you. And so there's always like a balanced trade-off between how do you make the model to be more helpful for our users, but also not being harmful in other scenarios. So it's always about like, how do you make the model like more robust and like operate across like variety of diverse scenarios. That is so funny. I never thought about that. Most of the data that's trained on is kind of like assuming it's like a human describing the world and how they operate. And there's, it assumes there's a body and you can do things in the model until you don't have a body. Yeah. Okay. I want to talk a little bit about data while we're on this topic. I know you have strong opinions here. There's kind of this meme that models are going to stop getting smarter because they're running out of data. They're trained in a large part on the internet and there's only one internet and they've already been trained on it. What more can you show them about the world? And there's this trend of synthetic data, this term synthetic data. What is synthetic data? Why do you think it's important? Do you think it's going to work? I think there are two questions here. We can unpack one at a time, but people say you're hitting the data wall. I think people think more in the terms of like pre-trained large models that are trained on the entire internet to predict the next token. But what actually the model is learning during that process is actually how do you compress the compression algorithm here? The model learns to compress a lot of knowledge and it learns how to model the world. So the next prediction of the world, teach me how to drive basically. And you only have a few words that will match that, a car. So the model actually learns about the world in itself. So it's like, it's modeling human behavior. Sometimes it's modeling. And when you talk to like pre-trained models, which are very, very large, they're actually extremely diverse and extremely creative because you can talk to almost any Reddit user through a pre-trained model. But I think what's happening right now with like new paradigm of like O1 series is of like the scaling in post-training itself is not hitting the wall. And that's because basically we went from like raw data sets from pre-trained models to infinite amount of tasks that you can teach the model in the post-training world via reinforcement learning. So any task, for example, like how to search the web, how to use the computer, how to write, wow, like all sorts of tasks that you're like trying to teach the model, all the different skills. And that's why we think like there's no data wall or whatever, because there will be infinite amount of tasks. And that's how the model becomes extremely super diligent. And we are actually getting saturated in all benchmarks. So I think the bottleneck is actually in evaluations that you don't have all the frontier, like EVAs, like, I don't know, GPQA, which is like a Google-proof fashion answering, like PhD level. Intelligent benchmark is like getting to like, I don't know, more than like 60, 70%, which is what PhD gets. So it's like literally hitting the wall in like EVAs. I want to follow both those threads. So the first is on this idea of synthetic data, is a simple way to understand it that the models are generating the data that future models are trained on and you ask it to generate all these ways of doing stuff, all these tasks, as you described, and then the newer models trained on this data that the previous model generated. Some tasks are synthetically curated. So this is like an active research area is like how do you, can you synthetically construct like new tasks with model to like learn. Sometimes, you know, like when you develop products, you got a lot of like data from the product and like user feedback and you can use that.this post-training world, sometimes you still want to use human data because actually some of the tasks can be really, really hard to teach. Experts only know certain knowledge about some chemicals, or biological knowledge. So you actually need to tap into the expert knowledge a lot. So yeah, I think to me, like synthetic data training is more for like product, it's like a rapid model iteration for similar product outcomes. And we can dive more into but the way we made Canvas and tasks and like new like product features for HTTP was mostly done by synthetic training. Let's actually get into that. That's really interesting. I want to talk about evals. But let's follow that thread. So talk about how this helped you create Canvas. So when I first came to OpenAI, I really had this idea of like, okay, like it would be really cool for ChatterBot to actually like change the visual interface, but also change like the way it is with people. So going from like, being a chatbot to more of a collaborative agent, and the collaborator, it is like a, it's like a step towards like, more gigantic systems that become like innovators, ultimately. And so the entire team of like applied engineers, designers, products, like research, kind of like got like formed in the air, almost out of like nothing, it's just like a collection of people who just like got together, and the rapidly started iterating with each other. Actually, like Canvas is like one of the, I would say like the first project at OpenAI where researchers and applied engineers started working together from the very beginning of the product development cycle. And I think like, there's a lot of things that we have learned on the way. But I definitely came to with the mindset of like, we need to do like a really rapid model iteration, such that like, it will be much easier for engineers to, you know, work with the latest model possible, but also learn from like, user feedback, or like early like internal dogfood, how do we improve the model very rapidly. And, you know, it's really hard to like, kind of like figure out like how people when you deploy a product, how people would be able to like use it. And so like, the way you synthetically train the model is basically figuring out like, what are the most core behaviors that you wanted this product feature to do? And for Canvas, for example, it was it came down to like three main behaviors. It was how do you trigger Canvas for prompts like, write me a long essay, when the user intention is mostly like iterating over long documents, or write me a piece of code, or when to not trigger Canvas for prompts like, can you tell me more about president? Like, I don't know, some of the general questions. So you don't want to trigger Canvas because the user intention is mostly getting answer and not necessarily like iterate over a long document. The second behavior is how do you how do we teach the model to update the document when the user asks? So one of the behaviors of the model is actually have like the some agency and autonomy to literally go to the document and like select specific sections and either delete it or edit so highlighted and rewrite certain sections. So sometimes the model, sometimes the user would just like say, change the second paragraph to be something friendlier. And you would have to teach the model to literally find the second paragraph in the document and change it to a friendly tone. So basically, you teach both like how to trigger like, edit itself, but also how do you teach the model to get higher quality edits for the document. In case of like coding, for example, there's also like the question of like, how good the model is like completely rewriting the document versus like having like very specific targeted edits. So that's like another like layer of decision boundary within like edit itself. It's like select the entire document and like rewrite completely or you want to like have like very targeted custom behavior. And you know, like when you first launched the model, we would bias the model towards like more rewrites, because we saw the quality of the rewrites were like much higher. But over time, you're like kind of shifting based on like user feedback and what they are learning from each of the deployment. Lastly, this the third behavior that we taught synthetically, the model is how to make comments on any document. So the way we use that is like, we would use a one model to produce to like simulate like user conversation, let's say like, write me a document about x, y, z, but then we use a one to like produce the document. And then the kind of injunctive like user prompt to be like, Oh, make some comments critique my piece of writing, or critique this piece of writing that you just made. And then we taught the model to like make comments on the document on like very specific document. This is like, also like what kind of comments you want the model to make? Like, do they make sense or not? Like, how do you teach the quality of that? And it all came down to like measuring progress via very robustBut yeah, this is how you would use like a while in the like kind of synthetic data generation for like the screening. Okay, this is so interesting. So you talk about this idea of teaching the model and you mentioned how it's using synthetic data to teach the model different behaviors. Is a simple way to think about it. Basically, that's where you do that by showing it what success looks like using basically evals. Is that the simple way to think about it? Like, here's what you doing this successfully would look like. And that teaches it. Okay, I see. This is what I see. Yeah, great. Yeah. Amazing. Yeah, you got it. Okay, got it. I want to start unpacking what your day to day looks like as you're building these sort of things. Is it like you sitting there talking to some version of ChatGPT, crafting these evals? Sometimes I do that. Sometimes I do sit with ChatGPT. Actually, I think I learned this so much from Andrade. It's like, people spend so much time just like prompting models and like quality and all that all the time. And you actually get a lot of new ideas. How do you make the model better? It's like, oh, like this is this response is kind of weird. Like, why is it doing this? And you start like debugging or something or like, you start like figuring out like new methods. Like, how do you teach the model to respond in a different way? Like have better personality, let's say. So it's the same thing of like, how personality is made like in the models within those, like very similar methods. But yes, I think my time at OpenAI have changed. I think when I first came, I was like, mostly like research IC work. So I was like, building a lot of like, I was like writing code, like, you know, changing models, writing evals, working with PMs and like designers to like, learn, teach them how to like, even think about like evaluations. I think that was like, really cool experience. And I think it was like an adoption of like, how do you like do this, like, prior management of like AI features or like AI models? Um, yeah, but now it's like, mostly like, you know, like management and like mentorship. I'm still like, doing I see like research code after like 4pm, although, but yeah, it's kind of like changed. All right, don't talk too much about being a manager, because everyone's firing their managers. Who needs managers anymore? That's what I hear now. Just kidding. It's interesting that so much of your time was spent on teaching product teams how evals integrate and how important that is. And I've heard this a few times, and I haven't personally experienced it yet. So I think it's an important thread to follow is just how writing these evaluations is going to become increasingly an important part of the job of product teams, especially when they're building AI features and working with them. So can you just talk a bit more about what that looks like? Is it like sitting there with an Excel spreadsheet, basically showing like, here's the input, here's the output, here's how good the result was talking about what that actually looks like very practically. It certainly depends on what you're developing. But there are various types of like evaluations. So sometimes I do ask product managers, or there's also like new role that we have like model designers to kind of like go through some of the user feedback, maybe or like think of like various like user conversations that should have triggered, like under this circumstances, it should trigger Canvas. And then you have this like ground truth label of like, okay, with this conversation, it should trigger Canvas, under this conversation, it should not trigger Canvas. And you have this like very binary deterministic kind of eval that for like, this is about behaviors like this, when we were launching tasks, for example, like, how do you make correct schedules is like actually really hard for the model. But we built out like some of the deterministic evaluations that is like, okay, like, if the user says like 7pm, it's like, the model should say 7pm. So you can like have a deterministic eval, so it's like pass or fail. So yeah, like the way it works is like, sometimes I ask product managers to like, go create like a rule sheet, like have different tabs. And like, what's the current behavior? What's like the ideal behavior? And like, why or like some notes. And sometimes we usually use it for eval, sometimes we use it for training. Because like, if you give this spreadsheet to like a one model, it can probably figure out like how to teach itself a good behavior. And I think there are certain type of like evals that is kind of more prevalent is like human evaluation. And you can have specific trainers, or you can have like internal people to when you have like a conversation of the prompt, and then you have like various completion of models, you can choose the win rate, which model is the best, which model produced the highest quality comment or edit, and then you can have like continuous win rates. And as you develop new models, it should always like win over the previous models. So it depends on what you want to measure. So interesting, like, basically, what I'm hearing, and that's something I'm learning about, as I talk to people is product development start might move from this, like, here's a spec PRD, let's build it together. And then cool, let's review it. Are we happy with this too, from that to, hey, AI, build this thing for me. And here's what correct looks like. And I'm spending all my time on what is correct look like on evals, essentially. You definitely want to like, measure progress of the model. And this is where evals is, is because like, you can have prompted model as a baseline already. And if the most robust evals is the onewere prompted baselines, get the lowest score or something. And then because then you know, like, okay, if you're trained a good model, then it should like, just like he'll climb on that eval all the time, while not like also like regressing on like other intelligence evals. So like, I think it's more what that's, that's what I'm saying, like, it's more of an R than science is like, okay, like, if you optimize the model for this behavior, like, you kind of don't want to like bring damage in like other areas of intelligence, or this is happening, like, all the time in every lab and every like, research team, I would say, like, prompting is like also a way to like, prototype, like new product ideas. Like early days at Andorra, when I was working like file uploads feature, I remember just like, you know, prompting the model to just like, I mean, when we were like launching like 100 key contexts, I was just like prototyping this in my local local browser. I did the demo, like people really, really loved it. And they just like wanted like API for like file uploads or something. And then that's when it clicks to me like, I also like wrote a blog post, like, it comes to me like prompting is like a new way of like product development or like prototyping for designers, and for like product managers, for example, one of the features that I wanted to do is like have a personalized, recommend a personalized starter prompts. So whenever you come to like cloud, like, it should like, recommend you like starter prompts based on what your interests are. And so like, you can literally do it like prompting for that. Another feature was like generating titles for the conversations. It's a very small, like micro experience, but I'm really proud of the way we did that was because we took like five latest conversation from the user, like ask the model, like what's the style of the user. And then like for the next kind of new position, the generated title will be of the same like style. It's just like really little like micro experiences like this. That's so cool. Did you do that at Thropic or at OpenAI? At Thropic. Okay, cool. I love the file upload feature that cloud has, by the way, OpenChat GPT doesn't have that yet. Is that right? Oh, I think it has. I think like the way it's implemented is like very different, though. Okay, maybe it's the PDF feature because I use it all the time with cloud. Okay, that's cool. Someone needs to get on that. Man, it's wild how many features you built that I use every day and that many people use every day. This prototyping point you made is really important. It's something that comes up a ton on this podcast. Also, how that is maybe the way that AI has most impacted the job of product builders recently is just prototyping instead of going from showing just like here's a PRD, here's a design. PMs more and more just here's the prototype of the idea that I have and it's working. You can play with it. Yeah. Yeah. Okay. I want to spend a little more time on how you operate. So you talked about you built this in launch list tasks features. Is that the way you describe your tasks? Yeah. So talk about how that emerged and let's better understand just how you collaborate with product teams and how OpenAI works in that way. Whatever you can share there. I think Canvas and tasks are going into the bucket of projects where it's like more like short or medium terms. And actually the way Canvas and tasks came about to be was like, it started with one person prototyping and creating a spec. It's kind of like PRD. It's creating a spec of the behavior of the model. I don't think tasks is extremely groundbreaking feature necessarily. What makes it really cool is because the models are so general, model can now search. They can write sci-fi stories. They can search for stocks. They can summarize the news every day because the models are so general. Giving something familiar to people that like notifications is very familiar. Having reminders is very familiar. So creating form factor for the people who are very familiar with Canvas. Google Docs is very familiar. But then you add magical AI moment and it becomes very powerful. But the way it comes usually operationally, it starts as a prototype, literally prompted prototype of how you would want the model to behave. For tasks, for example, you kind of need to design. A little bit like systems design thinking is like, okay, well, if the user says, remind me to go to lunch at 8am tomorrow. What kind of information does the model needs to extract from that prompt in order to create a reminder? And so this is how you design a stack for a new feature, like a tool. Canvas and tasks are all tools. So it's like, how do you create the tool stack? And then it's mostly like, developing JSON schema was like, okay, like from this prompt, maybe the model should extract like, the time that the user requested. And then you're thinking about like, which which one might you want the time to be? And then like, how do you want the model to like, notify you is like, basically, is the user should give instruction to the model. And then this instruction would like fire off like every day or something at that particular time. So for example, if you say like, search, like every day, I want to like learn now about the latest AI news. The model should rewrite into like, okay, like search for the latest AI news. And this will this task will get fired at that particulartype of the model that the user requested. And then, you know, like your design is like tool spec. And then actually, I don't know, like, I feel like sometimes, like, it's like through conversations, I, like, I don't like people ask me to like, join the team. And they're like, Oh, my God, like, we need to be searchers. So like, we need like some support, like, we need like to train the models. Or sometimes like, Canvas is like, mostly like, I just pitched the idea of like, it got staffed quite immediately during the break. So I know, like, it depends on the project. And usually with staffing, it's like mostly like a product manager, model designer, actual product designer, a couple of researchers on a bunch of like applied engineers, depends on the complexity of the project. And then like, you know, for tasks, it took like, I don't know, like, two months or so to go from like zero to one basically. For Canvas, this was like, four or five months, I guess, to go from zero to one. But yeah, and I'm like, you know, you teach product managers how to like build evals. And like, maybe, you know, how do we not only like ship the better feature, but how do we think like, longer term, like, what kind of like cool features that you want tasks to have? Like, I think it would be nice for tasks to be like, a little bit more personalized. It'd be nice to have like, to create tasks via voice and on a mobile, right? Like, so you kind of need to like, this is how you get like research roadmap right here is like, thinking like how the feature will be developed in the future. And then from there, it's like, you like start getting data sets, like, with evals, you want to make sure that goes well. And then like, you need to have like a trade off between like, what methods you want to use. And the reason why I really love like, synthetic, like relying purely on synthetic data, instead of like collecting data from humans is because it's like much more scalable. It's cheap, it doesn't have like, you literally sample from the model. And you teach the core behaviors of the models. And that will generalize to all sorts of diverse coverage. And when you launch the beta feature, you learn so much from the users that you can like, all your synthetic sets can be shifted in the distribution of how the users behave in the private camera. And this is how you improve. And this is what happened to Canvas too, when we launched from beta to GA. This episode is brought to you by Loom. Loom lets you record your screen, your camera and your voice to share video messages easily. Record a Loom and send it out with just a link to gather feedback, add context or share an update. So now you can delete that novel length email that you were writing. Instead, you can record your screen and share your message faster. Loom can help you have fewer meetings and make the meetings that you do have much more productive. Meetings start with everyone on the same page and end early. Problem solved, time saved. We know that everyone isn't a one take wonder when it comes to recording videos. So Loom comes with easy editing and AI features to help you record once and get back to the work that counts. Save time, align your team, stay connected and get more done with Loom. Now part of Atlassian, the makers of Jira. Try Loom for free today at loom.com slash Lenny. That's L-O-O-M dot com slash Lenny. Something that I want to help people understand, and I don't even 100% understand this is what's the simplest way to understand the job of a researcher versus, say, a model designer and other folks involved? Like, what's the simplest way to understand what researchers do at OpenAI? So the approach that I described, I mostly like product oriented research, it's mostly product research. Another part component of my team is actually more like longer term exploratory projects. And it's more about like, developing new methods, understanding those methods, under a variety of circumstances. So like basically developing methods, you kind of like need to follow very similar kind of like recipe of like building evals, but it's like more sophisticated evals, like you kind of want to have like auto distribution or like, if you want to measure generalization, you kind of need to like after that. But it's basically more sciencey in a way where, you know, if we talk about synthetic data, like one of the hardest things about synthetic data is like, how do you make it like more diverse? Diversity in synthetic data is like one of the most important questions right now. And so it's like, exploring like ways to check like diversity as a general method that will work for all is like one of the research explorations. Other ones is like more like developing new capabilities. I feel like it's always about like, you know, like you, you work on this like new methods, and you have like signs of life that it's working. Either you think of like, how do you make it more general? Or do you think of like, how do you make it very useful? Or like, and this is how like longer term projects become more like medium and short term projects. That makes sense. Essentially working on developing ways to make the model smarter. Oh, four or five or six ways to like a one was a big breakthrough, right? The way it operates where it's not just here's your answer, it actually thinks and has right takes time to think through the process of coming up with an answer. Okay. Yeah, very helpful. Speaking of that, of thinking about the future where things are going, I want to spend some time on just this insight that basically you are building the cutting edge of AI, like at the very bleeding edge of where AI is going and where it is. And so I'm very curious to hear just your take on how you think things are going to change in the world, and how people work based on where you see things are going.I know it's a broad question, but let's say in the next three years, how do you see the world changing? How do you see people's way of working changing? It's a very humbling experience to be in both labs, I guess. To me, when I first came to Andarban, I was like, oh my God, I really love front-end engineering. And then the reason why I switched to research is because I realized at that time, it's like, oh my God, cloud is getting better at front-end. Cloud is getting better at coding. I think cloud can develop new apps or something. And so it can develop new features for the thing that I'm working. It was kind of like this meta-realization where it's like, oh my God, the world is actually changing. And when we first launched 100K Context at that time, obviously, I'm thinking about form factors that's like, yeah, file uploads were very natural, very familiar to people. But you can imagine we could just make infinite chats in the clouded AI app, right? As if it's 100K Context. But because file uploads, it's like form follows function, it's like the form factor of the file uploads kind of enable people to just literally upload anything, the books, any reports, financial, and ask any task to the model. And I remember it was like enterprise customers, financial customers are really interested in that. It's like, oh, wow, it's actually one of the very common tasks that people do in that setting. It was kind of crazy to see how some of the redundant tasks are getting automated, basically, by these smart models. And we're entering the era where I actually don't know, for example, sometimes if L1 gives me the correct answer or not, because I'm not an expert in that field. And it's like, I don't even know how to verify the outputs of the models. It's because all my experts know, they can verify those. So, yes, so basically, there are trends that are going on. The first trend is the cost of reasoning and intelligence is drastically going down. I had a blog post about this. Maybe I should update them like latest benchmarks, because at that time, like everybody was like doing like one benchmark and they'd be like, hookly saturated the benchmark. So like now we need to like do the same plot, but with another like frontier eval. But the cost of intelligence is like going down because it becomes like much cheaper. Smart, small models are becoming even smarter than like large models. And that's because of like the distillation research. This happened with like Clouty Haiku. I was like working on like post-training on Clouty Haiku and I realized it was much smarter than like Cloutool, which was like way bigger or something like that. But like the power of like small models become very intelligent and fast and cheap. We are moving towards a world that has like multiple implications, but that means that like people will have more access to AI and that's really good. Like builders and developers will have much better access to AI. But also it means like all the work that has been like bottlenecked by intelligence will be kind of like unblocked. So anyone like I'm thinking about healthcare, right? Like if I have instead of going to a doctor, I can like ask ChaiGPT or give ChaiGPT a list of symptoms and ask me like, oh, which like, would I have like a cold, flu or like something else? Like I can literally get the access to like a doctor almost. And there's like been some research studies around that. Yeah, there's a New York Times story about that where they compared doctors to doctors using ChaiGPT to just ChaiGPT and just ChaiGPT was the best of them all. Like doctors made it worse. Yeah. Yeah, that's crazy. Like education, I think I would have dreamt if like I had the tool like ChaiGPT when I was like young and like would learn so much. But it's like people can now learn almost anything from these models. So they can learn new language. They can learn how to build new look up like anything that you want. And like I'm so like it's humbling to like have like launch Canvas and like bring that thing to the people, enable them to do something else that they couldn't have ever before. And I think this is there's something like magical around this experience as education will have massive implications like I guess like scientific research, right? Like I think it's like the dream of like any AI research is like automated AI research. It's kind of scary, I'd say, which makes me think that like people management will say, you know, it's like one of the hardest things to it's like emotional intelligence with the models like creativity in itself is like one of the hardest things. So writers, I don't think like people should be worried as much. I think it's like I think it alleviates a lot of like redundant tasks for people. This is awesome. OK, I want to follow this thread for sure. And it's funny that what you described is like you were an engineer at Anthropic and you're like, OK, Claude is going to be very good at engineering. This isn't going to be a potentially career long term. So I'm going to move into research and the AI is going to need me for a long time to build it, to make it smarter. I would say we still have like I think Canvas team has still have like a really cool like front engineer.I don't think models are there yet, but if you can get the models to this top 1% or something. So what I want to move on to next along these lines, and this is just speculation, but what skills do you think will be most valuable going forward for product teams in particular? So if folks are listening and they're like, okay, this is scary, what should I be building now to help me stay ahead and not be in trouble down the road? What skills do you think are going to be more and more important to build? Yeah, I think creative thinking, you kind of want to generate a bunch of ideas and filter through them in order to build the best product experience. Listening, you want to build something that the most general model will not replace you. And oftentimes, you build something and you make it really, really good for a specific set of users. And actually, the mode is now in your user feedback. The mode is more in whether you listen to them, whether you can rapidly iterate. The mode is in here. I don't think we are yet to like, there are so many ideas, I think there's an abundance of ideas that you can work on. I wouldn't be worried. In fact, I do think people in AI fields are like, I wish they were a little bit more creative and connecting dots across different fields or something like that to develop really cool new generation and new paradigms of interactions with this AI. I don't think we've cracked this problem at all. A couple of years ago, I was telling some people, I was like, you kind of want to build for the future. Because it doesn't necessarily matter whether the model is good or not good right now. But you can build product ideas, such that by the time the models will be really good, it will work really well. I think it just happened naturally, like, for example, the Cloud Artifacts, and I feel like early days of Canvas was like, back in like 2022, like before Chachapiti, like writing ID was like on all Chachapiti. But I feel like Cloud 1.3 model itself was like not there to like made like really extreme good like high quality edits, for example, like coding. And I feel like I see like startups like Cursor, and it's like doing super well, like, unless because they like iterate so fast, they like invent like new ways, or like training models, they move really fast, they listen to like users, like massive distributions, like, yeah, it's kind of cool. That's really helpful, actually. So what I'm hearing is that soft skills, essentially, are going to be more and more important, powerful. You talked about management, leading people being creative and coming up with innovative insights, listening. There's a post I wrote that I'll link to right look, I try to analyze what AI, how AI will impact product management, and we're actually very aligned. And my sense was the same thing that soft skills are going to become more and more important. And the things that are going to be replaced as the hard skills, which is interesting, because usually people value the hard skills like coding, design, writing really well. And it's interesting that AI is actually really good at that, because it's taking a bunch of data, synthesizing it, and writing, creating a thing versus all these fuzzy things around of what influences, convinces people to do things and aligning and listening, like you said, creativity, anything along those lines come up as I say that? I think it's actually really, really hard to use the model how to be aesthetic, or like, do like visual, a really good like visual design, or like how to be extremely creative in the way to write. I think like, I still think like JGP kind of sucks at like writing. And that's because it's like, it's like bottlenecked by this like creative reasoning. I think like prioritization is like one of the most important like, I think like, for a manager, I feel like I actually like AI research progress is bottlenecked by like management, like research management is because you have like, constrained set of computes, and you need to like allocate the computes to the research paths that you feel the most convinced about. It was like, you need to like really, you need to have like, a really high conviction to put the compute and like, it's more like return on investment kind of situation as like, okay, yeah, like, I'm thinking a lot about like, like, okay, like, how do I cross all my projects, which projects are higher priorities, like prioritization, and also like on the lower levels, like, which experiments are really important to run right now, and which are not and like cut through the line. So I think like, prioritization, communication, like, management, people skills, like empathy, like, understanding people, like, kind of like collaboration, like, I think like, Canvas wouldn't be like an amazing launch, if it wasn't like about like, people. And I think it's a wonderful group of people, and like, I got a chance to like work with like people like Lee Byron, who's like a co creator, like GraphQL, and like some of the best like Apple designers, and it's like, so cool to like see, and like, how do you create this like collaboration between people, it's just like, something that's still humane, I think. Let me just follow through a little bit, because I imagine people listening are like, okay, but once we have AGI or SGI, it's like, it'll do all this, you know, it's like, there's a world where like,Why isn't all this done? I think it's easy to just assume all that. I'm curious this idea of creativity and listening, why you think AI isn't good at it, other than it's just very hard to train it to do this well. Is there anything there, just like why this is especially difficult for AI and LLMs to get good at? I think currently it's difficult for many reasons. I think it's still an active research area and it's something that I think my team is working on. It's like, okay, how do we teach the model to be more creative in the writing? I think this new paradigm of the models think more should actually lead to better writing in itself. But when it comes down to idea generation, or discriminating of what is a good visual design or not, I feel like it hasn't had learned examples from people to discriminate it very well. I do think it's because there are not that many people who are actually really, it's not accessible to models to learn from these people, I guess. So I guess that's why it sucks. Yeah, that makes sense. Basically, there's not enough of you yet, researchers teaching it to do these things slash people that have incredible taste and creativity that can teach these things. You could argue this will come, but we don't need to keep going down that thread. Let me ask you a specific question. In this post I wrote, I made this argument that a lot of people disagreed with, that strategy is something that AI tooling will become increasingly great at and take over. There's the sense that that's the thing that people will continue to be much better at, and you can't offload to AI, basically developing your strategy, telling you what to do to win. My case is, isn't strategy just take all the inputs, all the data you have available, understand the world around you, and come up with a plan to win? It feels like AI would be, like an LLM would be incredibly smart at this. What's your take? I think, again, you teach the model all sorts of tools and capabilities and reasoning, right? When it comes down to, for Canvas right now, it would be very cool for the model to aggregate all the feedback from users, summarize the top five most painful flows on user experiences, and then the model itself is very capable of thinking of knowing how it's being made, figure out how to create a data set for itself to train on it. I don't think we are far away from that kind of self-improvement, models becoming self-improved. Then the product development is basically self-improving its own organism or something. Again, strategy is more like data analysis and coming up with... I think what models are really good at is connecting the dots, I think. It's like, okay, if you have user feedback from this source, but you also have an internal dashboard with metrics, and then you have other kind of feedback or inputs, and then it can co-create a plan for you, recommendations even. I think this is one of the most common use cases for HTTP users coming up with these sorts of things. That makes sense. Essentially, a human can only comprehend so much information at once and look at so much data at once to synthesize takeaways. As you said, these context windows are huge now. Here's all the information. What's the most important thing I should do? Yeah, same as scientific research. Ideally, the model will be able to suggest ideas, new ideas, or iterate on the experiment, or given the empirical results of the previous experiments, how do you come up with new ideas or methods? Yeah. Oh, man. Okay, so just to close the loop on this conversation, this part of the thread is the skills you're suggesting people focus on building and leaning into, soft skills like creativity, managing influence, collaboration, looking for patterns. Is that generally where your mind is at? Yeah. I'm thinking a lot about how do we make our relations more effectively? I think this is mostly management, I guess. It's like, how do you organize research teams, or generally, teams combine, compose teams such that they will be maximally succeed at the maximum performance of what can possibly... We can literally create the next generation of computers. It's just the matter of conviction and the way you manage through that. It's scaling organizations or scaling product research, I guess. Yeah, I think you're basically building this thing and not efficiently doing it is like limiting the potential of the human species right now. It's mismanagement within the research team and OpenAI and Anthropic and some of these other models. Yeah, it's kind of crazy to think about. Holy moly. Okay, so speaking of Anthropic and OpenAI, you've worked at both. Very few people have worked at both companies and have seen how they operate. I'm curious just what you've noticed about the differences between these two, how they operate, how they think, how they approach stuff. What can you share along those lines? It's more similar than different. Obviously, there is a lot of... There are some differences also when it comes to nuances. I'd say culture, I really...I love Antarctic and I have a lot of friends there and I also love OpenAI and I still have a lot of friends there. It's not about enemies. I feel like in AI, it's all about the competitors and enemies. It's actually one big community of people doing the same thing. What I would have learned from Antarctic is this real care and craft towards model behavior, model cost, model training. And I've been thinking a lot about what makes Cloud, Cloud and what makes HTTP, HTTP. And it actually comes down to operational processes that kind of leads to the outputs to the model, the outputted model. And it's the reason why Cloud has so much more personality and is more like a librarian. I don't know. I don't know. I'm like visualizing a cloud being like a librarian. Very nerdy or something. It's because I feel like it's a reflection of the creators who are making this model and a lot of details around the character and the personality and whether the model should follow up on this question or not. What's the correct ethical behavior for the model in this scenario? It's a lot of craft and curated data sets. And this is where I learned that part of art, I guess, at Antarctic. I would say Antarctic is much smaller. When I joined, it was 70 people. When I left, it was 10 people. Obviously, the culture changed so much. I really enjoyed being early days startup wives and people knew each other as a family, but the culture shifted. I would say I learned from Antarctic that they're much better at focusing and prioritization and very, very hard core prioritization, I guess. And they need to do it. But I think OpenAI is much more innovative and much more risk takers in terms of product or research. I don't know. Your full-time job can be just teaching the model how to be creative writers. And there's some luxury in this research freedom that comes to scale, maybe. I don't know. But I feel like I have much more creative product freedom to do almost anything, I guess, within OpenAI. I've lost Chachapiti into Delusion. It's more like, yeah, probably bottom stock, I guess. That's how I was thinking about it. It feels like OpenAI is more bottoms up, distributed people bubble up ideas, try stuff. And that leads to more products launching. I imagine more things just kind of being tried versus more of a, let's just make sure everything we do is awesome and great and craft and thinking deeply about every investment. That's really interesting. I've never heard it described this way. Karina, we've covered so much ground. This is going to help a lot of people with so many ways of thinking about where the future is going. Before we get to our very exciting lightning round, I'm curious if there's anything else that you think might be helpful to share or get into. One of my regrets, I guess, when I was early days at Antwerp was that, I think there was some luxury of the time, pre-Chachapiti, to actually come in with a bunch of ideas and prototype almost every day. And I think we did a lot of cool ideas. Cloud and Slack was actually one of the first tool-use-y products. Cloud could operate in your workplace now. It's kind of cool when you add Cloud to summarize the thread. So maybe you have an entire conversation with someone and then you want a summary of what happened. You can still add Cloud to summarize this. Also, it was really fun to even iterate on the model itself. It's like when you just talk to the model in Slack forever. It created some social element. It's kind of, let me join me in this Discord. People learned so much about prompting and how to work with Cloud. One of the features that was early tasks prototype was every Monday, Cloud would just summarize the entire channel. Or every Friday, it would just summarize a bunch of channels and give the news about the organization or something. So it's kind of a really cool form factor. I think thinking about form factors is a really important question in AI. Especially, we haven't even figured out how do we create an awesome product experience with O-series models. It's like the paradigm between synchronous, real-time, give-an-answer paradigm into more asynchronous paradigm of agents working on the background. But then now the question is, the agents should build trust with you and trust builds over time, which is like with humans. And you start this collaboration, which is why this collaboration model was like, you and a model is so important, because you both trust and the model learns from your preferences, so that it can become more personalized. And it will start predicting the next option that you want to take on the computer or something. And it's kind of like more predictive, much more. We went from personal computers, like personal model, basically here. Why is it not a thing? That seems like such an obvious feature that every LM should have as a Slack bot version of them.Is that a thing I can have you install or is that not a thing right now? I know that Cloud and Slack was sunsetted in like 2023 or something, but I think it was like after Chai GPT, it was mostly like the focus on like consumer use cases or like enterprise use cases. I think the form factor of like Cloud and Slack is like, it was kind of constrained a little bit when you wanted to develop new features. I want that. I know that Chai GPT had like Slack bar too, so I don't know, like maybe it will come back sometime. Yeah. I would pay for that. Any other memories from that time of early days? Because that's a really special place to have been as early days anthropic. Any other memories or stories from that time that might be interesting to share? I think the very first launch when we felt like when Clip Stem Use, again, was like 100 key context launch is like when the models could input the entire like book and give you like summary of the book or something or the entire financial or like have like multi files, financial reports, and then like give you an answer to the question, to very specific question. I think there was something in there that kind of like, oh my God, this is like a really cool new capability, not like model capability, but more like the capabilities that came from the product form factor itself rather than like the model capability as much. I think like other prototypes that we were thinking about, like, you know, like there's like one prototype of cloud workspaces, and it's like kind of the same idea, like cloud and I would have the shared workspace and that shared workspace is like a document and we can like tweet in the document. And I feel like sometimes the ideas, like private ideas lag, and they lag for like two years, just like in this case. It's interesting, there's these milestones that kind of open up our view of what is happening and where things are going. ChatGPT, I think was the first of just like, wow, this is much better than I would have thought. You talked about 100k context windows where you could upload a book and ask a question and have it summarized. I actually use that all the time when I have interview guests and they wrote a book. I sometimes don't have time to read the whole book, so I use it to help me understand what the most interesting parts are. And then I actually dive into the book, just to be clear. And then I don't know, maybe like voice was another one where you could talk to say ChatGPT. Are there any other moments there that you're like, wow, this is much better than I thought it was going to be? Yeah, I think like the computer use agents, like the model operating the desktop. And you can essentially think of like, you know, new kind of like, experience where the model can learn the way you browse. And from that preference, it can just like browse as just like you, and it's kind of simulated persona. And it's actually very similar to the idea of like, okay, like, maybe Sam Altman doesn't have a lot of like, time, maybe I want to like talk to like his simulated, like his simulation and ask, like, oh, like, for example, like, yeah, like, I really appreciate some of the technical mentorship from like, Jakob, like, but he doesn't have a lot of time. So it's like, I really want to like ask him these questions, like, how do you respond? Like simulated environments like this would be really cool. That's a great place to plug LennyBot. I have one of those. It's trained on all of my podcasts and newsletters. And it sits on many models. I don't know which one exactly they use, but it's exactly that. And it's not even me, it's all the guests that have been on the podcast on the newsletter I wrote. And you could just ask it, how do I grow my product? How do I develop a strategy? And it's actually shockingly good. Do you feel like it reflects who you are? The best part of it is you can talk to it. It's built, there's an 11 Labs voice version that's trained on my voice on this podcast. And it's actually very good. And people like have told me they sit there for hours talking to it. And somebody told it, interview me like I am on Lenny's podcast, ask me questions about my career. And he did a half hour podcast episode with LennyBot. That's so fun. It's incredible. Future is wild. Yeah, I think like content transformation is like, you know, like I would imagine sometime like, you know, when you generate a sci-fi story in Canvas, like you can like transform this into like audio book, like where you have like very natural like content transformation from one media to another medium. I think like one of my earliest inspiration is like one of the last episodes of like Westworld, where I don't want to spoil, but where Dolores comes to her work at the time. And she comes to like this like new workspace and she starts like writing a story. And then as she writes a story, like a 3D like virtual reality starts like creating on the fly. So I kind of want to create that. Kind of cool. Wow. Speaking of medium, I guess I was wondering if I should go in this direction or not. But real quick, Kevin Weil, slash Kevin Weal, I don't know exactly how to pronounce his last name. The CPO of OpenAI. Is it Weil or Weal? I think Weal. Weal. OK, OK. Let's just say that. He was he did a panel at the Lenny and Friends Summit last year, and he made this really fascinating point that chat is a really interesting interface for these tools because they're just getting smarter and smarter and smarter and smarter and smarter. And chat continues to work as a paradigm to interact with them.Similar to a human. You could talk to Albert Einstein, you could talk to someone not very smart. And it's all conversation still. And so it's a really flexible way to interact with increasingly good intelligence. At some point, it'll not be so great. And you're talking about all these ways that you're adding additional ways to interact. But it's interesting chat proved to be a really powerful layer on top of all this stuff. Yeah, that's really cool. I feel like chat also has a social element, which is like very humane. It's like, you know, you sometimes want to like get into a group chat and like, yeah, having conversations with AI is kind of like a group chat in itself. It's a massive thing. I saw this, this idea of like, how do you build like features like this? Like, I see tasks as like, this, like, general kind of like, feature that will scale very nicely as the models would develop like new capabilities ourselves. It's like, like the model will be able to like do better like searches and like, you know, create new like, come up with like more creative, like writing on like render, you know, React apps and like HTML preview, like apps and like you can have like every day a new puzzle for you like every day, like continue the story from the previous day. It's like, it scales very nicely. You mentioned something as we were getting into this extra section that we ended up going down is this idea of your agents using a computer. I know this is actually something you are going to launch today, the day we're recording it, which will be out by the time this comes out, called Operator. Can you talk about this very cool feature that people will have access to? Yeah, so I unfortunately did not work on that, but I'm really, really excited about like this launch. It's basically an agent that can complete the task in its own like virtual computer, like in its own virtual environment, you can do any literally task like, order me a book on Amazon. And then ideally, the model will either like follow up with you, like which book do you want? Or like know you so well, they will like start recommending like, oh, here's the five books that I might recommend you to buy. And then like you hit like, yeah, help me, help me buy and then the model goes off into its own virtual little browser and like complete the task and buy the book on Amazon. And then if you give the model like credentials, credit cards, obviously it comes with like a lot of trust and like safety, then it will just complete the thing for you. It's a virtual assistant. It's interesting how this just sounds like obviously this should happen. Like why is this not yet a thing, which is also mind blowing that we're just assuming this should exist, like just some AI doing things for you on a computer. You just have to do like, it's absurd. It's actually really hard. And I think like, um, you're still cracking this. I feel like, I don't know if you use like tupple. It's like a pair programming. No. But I don't know if you love programming. So if you shopify uses this, I remember came up on a podcast. Oh, nice. Yeah. So it's a very cool product where you can just call anyone at any time, and then like share screen and the other person can like have access to the screen and like start like, literally operating your computer. And it's very like real time, like diligence is like very, it's like very high quality. And it's just like, I kind of want the same. It's like I want to pair program with like my model. And like the model should even talk to me like draw like very specific like section in my code and VS code and like, tell me like, I will teach me and you can have like different modes. It's like right here. It was like a product right here for you. I don't know. People, some people should build that. Sounds like a startup just got birthed. Yes. I'm still listening to this. You mentioned that it's very hard to do this agent controlling a computer as you and helping out what makes it so hard for whatever, however much you can explain briefly. Much of it is like, because right now the models operating on like pixels, instead of like language or whatnot, like pixels is actually really, really hard for models, just like perception or visual perception. I think there's still like a lot of like multimodal research that's going on. But I think like language scales so much like easier compared to like multimodals because of that. Another thing that I just like my team is working on is like, how do you derive human intent very correctly? It's like, sometimes like, does the model know enough information to ask a question or like to complete the task? You kind of don't want like an agent to like go off for like 10 minutes and then come back with like an answer that you didn't even want. That actually creates like much more versed user experience. And this comes with like teaching the model, like, like people skills. It's like, you know, like, what do people like? Like kind of like creating like the mental model of the user and like care about the user in order to ask certain questions. Like actually that part is like hard to the models. That relates to what we talked about earlier, where there's kind of the soft skill people skills pieces, not where these models are strong yet. Okay, I'm gonna skip the lightning round. I want to ask just one question from the lightning round. Something funny. Okay, so when AI replaces your job, Karina, I'm curious what you're gonna gives you a stipend gives you a monthly stipend. Here's your here's your salary for the month. What would you want to do? What do you want to spend your time on? What will you be doing in this future world? I've been thinking about this all the time.I feel like I have a lot of job options. I would love to be a writer, I think. I think that'd be super cool. Just to like write like, short stories, like sci-fi stories, novels. I really like art history. So you know, it's like a conservationist to like in the museums, who just like, try to preserve like art paintings, but just like painting through a lot of I think that'd be really cool to do. Yeah. That sounds beautiful. I don't know. What I'm hearing is you need to nerf these models to not get very good at writing, so that you can continue. Although at that point, you don't need to do it from like, you don't need people to buy, you're just doing it for fun. So it doesn't even matter if they're incredibly good at writing, or art, art conservation. Oh, man, what an episode of our conversation. What a wild time we're living in. Karina, thank you so much for being here. Two final questions. Where can folks find you online if they want to reach out and follow up on anything? And how can listeners be useful to you? You can find me on Twitter. You can also shoot me an email on my website. And I'm, my team is hiring. And so like, I'm looking for research engineers, research scientists, as well as like machine learning engineers, like people who come from product engineers who want to like learn about model training. I'm actually hiring for like, my team, my team is called the Frontier Product Research. And the train models, we develop new methods, but for product oriented outcomes. What a place to work. Holy moly. What's the best way for people to apply for these very lucrative roles? I think you can shoot me a DM on Twitter, or I'm yet to create a job description. Okay, this is the job description. Or you can apply under like post training team. Yeah. Okay, you're gonna get a flood of DMS. I hope you're prepared. Karina, thank you so much for being here. This was incredible. Thank you so much, Lenny. Bye, everyone. Thank you so much for listening. If you found this valuable, you can subscribe to the show on Apple Podcasts, Spotify, or your favorite podcast app. Also, please consider giving us a rating or leaving a review as that really helps other listeners find the podcast. You can find all past episodes or learn more about the show at Lenny's podcast.com. See you in the next episode.

## **基本情報**

- 公開日：2025年02月17日
- 長さ：20:07

## **要約**

In this segment, the speaker discusses the outcomes of three experiments involving the Flunty 5 language model's lie detection capabilities. The results varied, with accuracy rates ranging from 50% to 80%. The experiments revealed that language models can classify statements as deceptive, aligning with machine learning benchmarks. However, these models struggle to generalize this knowledge across different contexts due to the contextual dependency of deceptive linguistic cues. Exposure to diverse examples during training enables language models to generalize effectively. While promising for real-world lie detection applications, further development and research are necessary before widespread integration of AI lie detection systems. The speaker envisions a future where this technology enhances security measures and social media interactions. The potential applications include identifying politicians' true beliefs, preempting false intentions at security checkpoints, and aiding in recruitment processes. This optimistic outlook invites listeners to imagine a future where AI lie detection technology plays a significant role in various aspects of society. Listening to this episode would provide insights into the challenges and possibilities of leveraging AI for detecting deception in the future.

## **目次**

00:15 Frequency of lying in daily life
01:10 Relevance of detecting lies in criminal investigations and personal relationships
03:33 Humans' ability to detect lies vs. AI detection
04:11 Training large-language models and detecting lies
06:30 Model Training and Testing
07:18 Experimental Results
08:38 Deception and language models in machine learning
10:15 National Security and AI technology
11:31 Trust and credibility in AI technology
12:00 Concerns about blind trust in AI lie detection technology
14:03 Balancing trust in AI technology and critical thinking

## **文字起こし**

This is something you won't like. But here, everyone is a liar. Don't take it too personally. What I mean is that lying is very common and it is now well established that we lie on a daily basis. Indeed, scientists have estimated that we tell around two lies per day, although of course it's not that easy to establish those numbers with certainty. And, well, I introduced myself. I'm Riccardo. I'm a psychologist and a PhD candidate. And for my research project, I study how good are people at detecting lies. Seems cool, right? But I'm not joking. And you might wonder why a psychologist was then invited to give a TED talk about AI. Well, I'm here today because I'm about to tell you how AI could be used to detect lies, and you will be very surprised by the answer. But, first of all, when is it relevant to detect lies? A first clear example that comes to my mind is in the criminal investigation field. Imagine you are a police officer and you want to interview a suspect, and the suspect is providing some information to you, and those information are actually leading to the next steps in the investigation. We certainly want to understand if the suspect is reliable or if they are trying to deceive us. Then another example comes to my mind, and I think this really affects all of us. So, please, raise your hands if you would like to know if your partner cheated on you. And don't be shy, because I know. Yeah, you see? It's very relevant. However, I have to say that we as humans are very bad at detecting lies. In fact, many studies have already confirmed that when people are asked to judge if someone is lying or not, without knowing much about that person or the context, people's accuracy is no better than the chance level. About the same as flipping a coin. You might also wonder if experts such as police officers, prosecutors, experts, and even psychologists are better at detecting lies. And the answer is complex, because experience alone doesn't seem to be enough to help detecting lies accurately. It might help, but it's not enough. To give you some numbers, in a well-known meta-analysis that previous scholars did in 2006, they found that naive judges' accuracy was on average around 54%. Experts perform only slightly better with an accuracy rate around 55%. Not that impressive, right? And those numbers actually come from the analysis of the results of 108 studies, meaning that these findings are quite robust. And, of course, the debate is also much more complicated than this and also more nuanced, but here the main take-home message is that humans are not good at detecting lies. What if we are creating an AI tool where everyone can detect what someone else is like? This is not possible yet, so please don't panic. But this is what we tried to do in a recent study that I did together with my brilliant colleagues, whom I need to thank, and, actually, to let you understand what we did in our study, I need to first introduce you to some technical concepts and to the main characters of this story. Large-language models. Large-language models are AI systems designed to generate outputs in natural language in a way that almost mimics human communication. If you're wondering how we teach these AI systems to detect lies, here is where something called fine-tuning comes in, but let's use a metaphor. Imagine large-language models being a student who has gone through years of school, learning a little bit about everything, such as language, concepts, facts, but when it's time for them to specialise, like in law school or in medical school, they need more focused training. Fine-tuning is that extra education. And, of course, large-language models don't learn as humans do, but this is just to give you the main idea. Then, as for training students, you need books, lectures, and examples. For training large-language models, you need data sets. And for our study, we consider three data sets, one about personal opinions, one about past autobiographical memories, and one about future intentions. These data sets were already available from previous studies and contain both truthful and deceptive statements. Typically, you collect these type of statements by asking participants to tell the truth or to lie about something. For example, if I was a participant in the truthful condition, and the task was, tell me about your past holidays, then I would tell the researcher about my previous holidays in Vietnam. Here, we have a slide to prove it.Per la condizione deceptiva, piperanno piceranno qualche di voi che non sono mai venuti a Vietnam, e vi chiederanno di creare una storia e convincere qualcuno altro che sei veroce venuto a Vietnam. E' asto è come funziona tipicamente. E come in ogni cursus universitario, potete sapere, dopo lezioni, hai examini. E' uguale, dopo trenirare i nostri modelli AI, vocemo testarli. E' la procedura che follo, che e' in realtà tipica, e' la follo. Quindi abbiamo scelto alcuni stati, di ogni dataset, e li abbiamo separati. Il modello non ha mai visto questi stati durante la fase di allenamento. E solo dopo che l'allenamento fosse completato, li abbiamo usati come un testo, come l'esame finale. Ma chi era il nostro studente allora? In questo caso, e' stato un modello di linguaggio grande sviluppato da Google e chiamato Flunty 5, Flunty per amici. E ora che abbiamo tutti i pezzi del processo insieme, possiamo in realtà scoprire in profondo il nostro studio. Il nostro studio è stato composto da tre esperimenti principali. Per l'esperimento primo, abbiamo finiturato il nostro modello, il Flunty 5, su ogni singolo dataset separatamente. Per l'esperimento secondo, abbiamo finiturato il nostro modello su due parti di dataset insieme, e lo abbiamo testato sull'ultimo, e abbiamo usato tutte le tre possibili combinazioni. Per l'esperimento finale, abbiamo finiturato il nostro modello su un nuovo test di allenamento più grande che abbiamo ottenuto combinando tutti i tre datasetti insieme. I risultati erano piuttosto interessanti, perché ciò che abbiamo trovato è che, nell'esperimento primo, Flunty 5 ha raggiunto un livello di accurazione tra il 70% e il 80%. Tuttavia, nell'esperimento secondo, Flunty 5 ha diminuito l'accurazione a quasi il 50%. E poi, sorprendentemente, nell'esperimento terzo, Flunty 5 è rimasto a quasi il 80%. Ma cosa significa questo? Cosa possiamo imparare da questi risultati? Dall'esperimento uno e tre, abbiamo imparato che i modelli di lingua possono effettivamente classificare statimenti come deceptive, sollevando i benchmark umani e allineandoci con i modelli di apprendimento macchina e di apprendimento profondo che gli studi precedenti hanno allenato sui datasetti insieme. Tuttavia, dal secondo esperimento, abbiamo visto che i modelli di lingua struggolano per generalizzare questa conoscenza, questo apprendimento, attraverso diversi contesti. E questo è apparentemente perché non c'è una sola regola universale di decezione che possiamo applicare facilmente in ogni contesto, ma gli usi linguistici di decezione sono dipendenti dal contesto. E dal terzo esperimento, abbiamo imparato che, in realtà, i modelli di lingua possono generalizzare bene attraverso diversi contesti, se solo hanno avuto una esposizione precedente a esempi durante la fase di allenamento. E credo che questo sia una buona notizia. Ma, mentre questo significa che i modelli di lingua possono essere applicati effettivamente per applicazioni di vita reale nella deteczione della luce, è necessario applicare di più, perché un solo studio non è mai sufficiente in modo che, da domani, possiamo tutti avere questi sistemi di AI sui nostri smartphone e iniziare a detectare le luci delle altre persone. Ma, come scienziato, ho una immaginazione vivida e vorrei sognare grande, e vorrei anche portarvi con me in questa giornata futurista per un po'. Quindi, per favore, immaginatevi con me vivendo in un mondo in cui questa tecnologia di deteczione della luce è ben integrata nella nostra vita, rendendo tutto, dalla sicurezza nazionale al social media, un po' più sicuro. E immaginatevi avere questo sistema di AI che potrebbe, in realtà, spostare opinioni false. Da domani, potremmo dire quando un politico in realtà sta dicendo una cosa e davvero crede in qualcos'altro. E come si tratta del contesto del board di sicurezza dove le persone sono chieste sulle loro intenzioni e le loro ragioni per le quali stanno crossando le frontiere o su board in place, si, su plane, beh, con questi sistemi potremmo, in realtà, spostare intenzioni false prima che neanche succedano. E come si tratta del processo di reclutamento? Ci abbiamo parlato di questo già. Ma, in realtà, le compagnie potrebbero eseguire questa AI per distinguere quelli che sono davvero appassionati del ruolo da quelli che stanno cercando di dire le cose giuste per ottenere il lavoro. E, in realtà, questa tecnologia di deteczione della luce sta cercando di dire le cose giuste per ottenere il lavoro. E, finalmente, abbiamo i social media. Scamperi cercando di distraggerti o di stenderti l'identità, tutto persino. E qualcuno potrebbe dire qualcosa sulle notizie false e, beh, perfettamente, il modello linguistico potrebbe automaticamente leggere le notizie, tagliarle come deceptive o false e potremmo anche fornire agli usatori un scorso di credibilità per l'informazione che leggono. Suona come un futuro brillante, giusto? Sì, ma tutto progresso brillante viene con rischi. Tanto come sono emozionato su questo futuro, penso che dobbiamo essere attenti. Se non siamo attenti,In my view, we could end up in a world where people might just blindly believe AI outputs. And I'm afraid this means that people will just be more likely to accuse others of lying just because an AI says so. And I'm not the only one with this view, because another study already proved it. In addition, if we totally rely on this lie detection technology to say someone else is lying or not, we risk losing another important key value in society. We lose trust. We won't need to trust people anymore because what we will do is just ask an AI to double check for us. But are we really willing to blindly believe AI and give up our critical thinking? I think that's the future we need to avoid. What I hope for the future is more interpretability. And I'm about to tell you what I mean. Similar to when we look at reviews online and we can both look at the total number of stars that place us, but also we can look at more in detail at the positive and negative reviews and try to understand what were the positive sides, but also what might have gone wrong to eventually create our own and personal idea if that is the place where we want to go, where we want to be. Likewise, imagine a world where AI doesn't just offer conclusions, but also provide clear and understandable explanations behind its decisions. And I envision a future where this lie detection technology wouldn't just provide us with a simple judgment, but also with clear explanations for why it thinks someone else is lying. And I would like a future where, yes, this lie detection technology is integrated in our life, or also AI technology in general, but still, at the same time, we are able to think critically and decide when we want to trust an AI judgment or when we want to question it. To conclude, I think the future of using AI for lie detection is not just about technological advancement, but about announcing our understanding and fostering trust. It's about developing tools that don't replace human judgments, but empower it, ensuring that we remain in the air. Let's step into a future with blind reliance on technology. Let's commit to deep understanding and ethical use, and we'll pursue the truth. Thank you.
